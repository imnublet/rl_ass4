{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e78a7aa-3e27-4d95-8c9e-19c137429c5a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Generator' from 'numpy.random' (/Users/sayf/anaconda3/lib/python3.8/site-packages/numpy/random/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-3-6bdff14d4c84>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      9\u001B[0m \u001B[0;31m# Imports\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     10\u001B[0m \u001B[0mget_ipython\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrun_line_magic\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'matplotlib'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'inline'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 11\u001B[0;31m \u001B[0;32mimport\u001B[0m \u001B[0mgym\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     12\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mpygame\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     13\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mmatplotlib\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpyplot\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mplt\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/lib/python3.8/site-packages/gym/__init__.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mgym\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mversion\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mVERSION\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0m__version__\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 4\u001B[0;31m from gym.core import (\n\u001B[0m\u001B[1;32m      5\u001B[0m     \u001B[0mEnv\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      6\u001B[0m     \u001B[0mWrapper\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/lib/python3.8/site-packages/gym/core.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mgym\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 7\u001B[0;31m \u001B[0;32mfrom\u001B[0m \u001B[0mgym\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mspaces\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      8\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mgym\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mutils\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mseeding\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      9\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mgym\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlogger\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mdeprecation\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/lib/python3.8/site-packages/gym/spaces/__init__.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0;32mfrom\u001B[0m \u001B[0mgym\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mspaces\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mspace\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mSpace\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mgym\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mspaces\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbox\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mBox\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mgym\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mspaces\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdiscrete\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mDiscrete\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mgym\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mspaces\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmulti_discrete\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mMultiDiscrete\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mgym\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mspaces\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmulti_binary\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mMultiBinary\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/lib/python3.8/site-packages/gym/spaces/space.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     13\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mnumpy\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     14\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 15\u001B[0;31m \u001B[0;32mfrom\u001B[0m \u001B[0mgym\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mutils\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mseeding\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     16\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     17\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/lib/python3.8/site-packages/gym/utils/seeding.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mnumpy\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 7\u001B[0;31m \u001B[0;32mfrom\u001B[0m \u001B[0mnumpy\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrandom\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mGenerator\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      8\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      9\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mgym\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0merror\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mImportError\u001B[0m: cannot import name 'Generator' from 'numpy.random' (/Users/sayf/anaconda3/lib/python3.8/site-packages/numpy/random/__init__.py)"
     ]
    }
   ],
   "source": [
    "# Install OpenAI Gym dependencies\n",
    "# !pip install Box2D\n",
    "# !pip install box2d-py\n",
    "# !pip install gym[all]\n",
    "# !pip install Box2D\n",
    "# !pip install box2d box2d-kengz\n",
    "# !pip install autograd\n",
    "# !pip install numpy\n",
    "# Imports\n",
    "%matplotlib inline\n",
    "import gym\n",
    "import pygame\n",
    "import matplotlib.pyplot as plt\n",
    "from gym.utils.play import play\n",
    "import numpy as np\n",
    "from autograd import grad\n",
    "from scipy.signal import savgol_filter\n",
    "\n",
    "# DL libraries\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.models import Sequential, load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2d6cfd-5840-40d4-9d9d-e96d22cd7d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComparisonPlot:\n",
    "\n",
    "    def __init__(self,title=None):\n",
    "        plt.rcParams[\"figure.figsize\"] = (7,5)\n",
    "        self.fig,self.ax = plt.subplots()\n",
    "        self.ax.set_xlabel('Episodes')\n",
    "        self.ax.set_ylabel('Average cumulative reward')\n",
    "        # self.ax.set_xscale('log')\n",
    "        if title is not None:\n",
    "            self.ax.set_title(title)\n",
    "\n",
    "    def add_curve(self,x,y,label=None):\n",
    "        ''' x: vector of parameter values\n",
    "        y: vector of associated mean reward for the parameter values in x\n",
    "        label: string to appear as label in plot legend '''\n",
    "        if label is not None:\n",
    "            self.ax.plot(x,y,label=label)\n",
    "        else:\n",
    "            self.ax.plot(x,y)\n",
    "\n",
    "    def save(self,name='test.png'):\n",
    "        ''' name: string for filename of saved figure '''\n",
    "        self.ax.legend()\n",
    "        self.fig.savefig(name,dpi=300)\n",
    "\n",
    "\n",
    "def smooth(y, window, poly=1):\n",
    "    '''\n",
    "    y: vector to be smoothed\n",
    "    window: size of the smoothing window '''\n",
    "    return savgol_filter(y,window,poly)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3aaf0c-9530-479f-b6b4-7e291bb98df7",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395865b2-e5b7-471d-a824-ff5a1977dbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'LunarLander-v2' # Create environment, source: https://www.gymlibrary.ml/environments\n",
    "env = gym.make(env_name) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc1abea-72d8-4393-a034-d063fcc8d51f",
   "metadata": {},
   "source": [
    "## Deep Q Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24e04ed-e910-44b7-89b1-2be5c3255690",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer(object):\n",
    "    \n",
    "    def __init__(self, max_size, input_shape, n_actions):\n",
    "        self.mem_size = max_size\n",
    "        self.mem_counter = 0\n",
    "        self.input_shape = input_shape\n",
    "        self.state_memory = np.zeros((self.mem_size, input_shape))\n",
    "        self.new_state_memory = np.zeros((self.mem_size, input_shape))\n",
    "        self.action_memory = np.zeros((self.mem_size, n_actions), dtype=np.int8)\n",
    "        self.reward_memory = np.zeros(self.mem_size)\n",
    "        self.terminal_memory = np.zeros(self.mem_size, dtype=np.float32)\n",
    "    \n",
    "    def store_transition(self, state, action, reward, new_state, done):\n",
    "        index = self.mem_counter % self.mem_size\n",
    "        self.state_memory[index] = state\n",
    "        self.new_state_memory[index] = new_state\n",
    "        self.reward_memory[index] = reward\n",
    "        self.terminal_memory[index] = 1 - int(done)\n",
    "        actions = np.zeros(self.action_memory.shape[1])\n",
    "        actions[action] = 1\n",
    "        self.action_memory[index] = actions\n",
    "        self.mem_counter += 1\n",
    "        \n",
    "    def sample_buffer(self, batch_size):\n",
    "        max_memory = min(self.mem_counter, self.mem_size)\n",
    "        batch = np.random.choice(max_memory, batch_size)\n",
    "        \n",
    "        states = self.state_memory[batch]\n",
    "        new_states = self.new_state_memory[batch]\n",
    "        rewards = self.reward_memory[batch]\n",
    "        actions = self.action_memory[batch]\n",
    "        terminal = self.terminal_memory[batch]\n",
    "        \n",
    "        return states, actions, rewards, new_states, terminal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f297609-01c3-43de-aa21-9ce7e04f1833",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dqn(lr, n_actions, input_dims, fc1_dims, fc2_dims):\n",
    "    model = Sequential([\n",
    "        Dense(fc1_dims, input_shape=(input_dims, )),\n",
    "        Activation('relu'),\n",
    "        Dense(fc2_dims),\n",
    "        Activation('relu'),\n",
    "        Dense(n_actions)\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    \n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffd7273-5c28-449c-b331-db8c860f6edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN_Agent(object):\n",
    "    def __init__(self, alpha, gamma, n_actions, epsilon, batch_size, input_dims, mem_size=1000000, min_epsilon=0.05, epsilon_decay=0.05):\n",
    "        np.random.RandomState(seed=None)\n",
    "        self.action_space = np.arange(n_actions)\n",
    "        self.gamma = gamma \n",
    "        self.epsilon = epsilon\n",
    "        self.alpha = alpha\n",
    "        self.batch_size = batch_size\n",
    "        self.memory = ReplayBuffer(mem_size, input_dims, n_actions)\n",
    "        self.q_network = build_dqn(alpha, n_actions, input_dims, fc1_dims=128, fc2_dims=128)\n",
    "        self.min_epsilon = min_epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        \n",
    "    def update_epsilon(self):\n",
    "        if self.epsilon <= self.min_epsilon:\n",
    "            self.epsilon = self.min_epsilon\n",
    "        else:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "    \n",
    "        \n",
    "    def remember(self, state, action, reward, new_state, done):\n",
    "        self.memory.store_transition(state, action, reward, new_state, done)\n",
    "        \n",
    "    def select_action(self,state):\n",
    "        state = state[np.newaxis, :]\n",
    "        \n",
    "        if np.random.random() < self.epsilon:\n",
    "            action = np.random.choice(self.action_space)\n",
    "        else:\n",
    "            actions = self.q_network.predict(state)\n",
    "            action = np.argmax(actions)\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    \n",
    "    def learn(self):\n",
    "        if self.memory.mem_counter < self.batch_size:\n",
    "            return\n",
    "        state, action, reward, new_state, done = self.memory.sample_buffer(self.batch_size)\n",
    "        \n",
    "        action_values = np.array(self.action_space, dtype=np.int8)\n",
    "        action_indices = np.dot(action, action_values)\n",
    "        \n",
    "        \n",
    "        q_vals = self.q_network.predict(state)\n",
    "        q_next = self.q_network.predict(new_state)\n",
    "        \n",
    "        q_target = q_vals.copy()\n",
    "        \n",
    "        batch_index = np.arange(self.batch_size, dtype=np.int32)\n",
    "        td = reward + self.gamma * np.argmax(q_vals, axis=1)*done\n",
    "        \n",
    "        q_target[batch_index, action_indices] = td \n",
    "        \n",
    "        _ = self.q_eval.fit(state, q_target, verbose=0)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f42482-0a44-4982-8fe0-1b2ee98fd38d",
   "metadata": {},
   "source": [
    "## Linear function approximation using Gradient SARSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ffa5ccf-4e1a-475f-9b72-879245b19391",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FA_Agent(object):\n",
    "    \n",
    "    def __init__(self, n_actions, state_dims, alpha, epsilon, gamma, epsilon_decay=0.9, min_epsilon=0.05):\n",
    "        np.random.RandomState(seed=None)\n",
    "        self.action_space = np.arange(n_actions)\n",
    "        self.gamma = gamma \n",
    "        self.epsilon = epsilon\n",
    "        self.alpha = alpha\n",
    "        self.state_dims = state_dims\n",
    "        self.weights = np.random.rand(self.state_dims, n_actions)\n",
    "        self.min_epsilon = min_epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "    \n",
    "    def update_epsilon(self):\n",
    "        if self.epsilon <= self.min_epsilon:\n",
    "            self.epsilon = self.min_epsilon\n",
    "        else:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "    \n",
    "    def approximate(self, weights, state, action):\n",
    "        return np.dot(state, weights)[action]\n",
    "    \n",
    "    def calc_grad(self, weights, state, action):\n",
    "        gradient = grad(self.approximate)\n",
    "        return gradient(weights, state, action)\n",
    "    \n",
    "    def select_action(self, state):\n",
    "        q_values = []\n",
    "        if np.random.random() < self.epsilon:\n",
    "            action = np.random.choice(self.action_space)\n",
    "        else:\n",
    "            for action in self.action_space:\n",
    "                q_values.append(self.approximate(self.weights, state, action))\n",
    "                action = np.argmax(q_values)\n",
    "        return action\n",
    "    \n",
    "    def update(self, state, next_state, action, next_action, reward, done):\n",
    "        q = self.approximate(self.weights, state, action)\n",
    "        q_next = self.approximate(self.weights, next_state, next_action)\n",
    "        q_grad = self.calc_grad(self.weights, state, action)\n",
    "        \n",
    "        if done:\n",
    "            self.weights += self.alpha * (reward - q) * q_grad\n",
    "        else:      \n",
    "            self.weights += self.alpha * (reward + self.gamma * q_next - q) * q_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2928ed03-ef73-454c-82bc-827340a97641",
   "metadata": {},
   "source": [
    "## Training and evaluating the LFA Agent on the Lunar Lander Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e41191-adc9-40b2-a273-d5d1d8a3d977",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rewards_per_rep = []\n",
    "avgs_from_last_hunderd = []\n",
    "max_timesteps = 1000\n",
    "reps = 5\n",
    "n_episodes = 600\n",
    "alphas = [0.0005, 0.001, 0.01]\n",
    "gamma = 0.99\n",
    "epsilons = [0.85, 0.8, 0.6]\n",
    "    \n",
    "def training_loop_SARSA_LFA(agent):\n",
    "    done = False\n",
    "    score = 0\n",
    "    state = env.reset()\n",
    "    timestep = 0\n",
    "    while not done:\n",
    "            if timestep == max_timesteps:\n",
    "                done = True\n",
    "            action = agent.select_action(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            score += reward\n",
    "            next_action = agent.select_action(next_state)\n",
    "            agent.update(state, next_state, action, next_action, reward, done)\n",
    "            state = next_state\n",
    "            timestep += 1\n",
    "            # env.render() # Uncomment for rendering\n",
    "    return score\n",
    "    \n",
    "\n",
    "for alpha in alphas:\n",
    "    for epsilon in epsilons:\n",
    "        for rep in range(reps):\n",
    "            agent = FA_Agent(n_actions=env.action_space.n, state_dims=env.observation_space.shape[0], alpha=alpha, epsilon=epsilon, gamma=0.99)\n",
    "            reward_per_ep = []\n",
    "            avg_scores_per_ep = []\n",
    "            for ep in range(n_episodes):\n",
    "                score = training_loop_SARSA_LFA(agent)\n",
    "                reward_per_ep.append(score)\n",
    "                avg_score = np.mean(reward_per_ep[max(0,ep-100):(ep+1)])\n",
    "                avg_scores_per_ep.append(avg_score)\n",
    "                if ep%10 == 0:\n",
    "                    agent.update_epsilon()\n",
    "                    # print('episode:', ep, 'score:', score, 'avg score:', avg_score)\n",
    "\n",
    "            rewards_per_rep.append(reward_per_ep)\n",
    "            avgs_from_last_hunderd.append(avg_score)\n",
    "            print('Finished rep:', rep)\n",
    "\n",
    "        avg_rewards_per_eps = np.mean(rewards_per_rep, axis=0)\n",
    "        all_avgs_from_last_hunderd = np.mean(avgs_from_last_hunderd, axis=0)\n",
    "\n",
    "        comparison_plot = ComparisonPlot(title=\"Reward per episode for a FA agent on the lunar lander task\")\n",
    "        comparison_plot.add_curve(np.arange(n_episodes), y=smooth(avg_rewards_per_eps, 10), label='Reward per episode, Alpha: %s epsilon: %s' % (alpha, epsilon))\n",
    "        comparison_plot.add_curve(np.arange(n_episodes), y=all_avgs_from_last_hunderd, label='Average reward from last 100 episodes, Alpha: %s' % (alpha, epsilon))\n",
    "        comparison_plot.save('fa alpha: %s, epsilon: %s', (alpha, epsilon))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f31538-a74a-4645-8932-bd89ac92bb15",
   "metadata": {},
   "source": [
    "## Training and evaluating the DQN Agent on the Lunar Lander Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4f5c154a-f9fa-4474-b2bb-851cb1568114",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'env' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-13-4cbbc5957cd6>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     30\u001B[0m     \u001B[0;32mfor\u001B[0m \u001B[0mepsilon\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mepsilons\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     31\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mrep\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mreps\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 32\u001B[0;31m             \u001B[0magent\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mDQN_Agent\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mgamma\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mgamma\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mepsilon\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mepsilon\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0malpha\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0malpha\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minput_dims\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0menv\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mobservation_space\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mn_actions\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0menv\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0maction_space\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mn\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbatch_size\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m64\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     33\u001B[0m             \u001B[0mreward_per_ep\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     34\u001B[0m             \u001B[0;32mfor\u001B[0m \u001B[0mep\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mn_episodes\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'env' is not defined"
     ]
    }
   ],
   "source": [
    "rewards_per_rep = []\n",
    "avgs_from_last_hunderd = []\n",
    "max_timesteps = 1000\n",
    "reps = 5\n",
    "n_episodes = 600\n",
    "alpha = [0.0005, 0.001, 0.01]\n",
    "gamma = 0.99\n",
    "epsilon = [0.85, 0.8, 0.6]\n",
    "\n",
    "def training_loop_DQN(agent):\n",
    "    done = False\n",
    "    score = 0\n",
    "    state = env.reset()\n",
    "    timestep = 0\n",
    "    while not done:\n",
    "            if timestep == max_timesteps:\n",
    "                done = True\n",
    "            action = agent.select_action(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            score += reward\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            next_action = agent.select_action(next_state)\n",
    "            agent.learn()\n",
    "            state = next_state\n",
    "            timestep += 1\n",
    "            # env.render() # Uncomment for rendering\n",
    "    return score\n",
    "\n",
    "for alpha in alphas:\n",
    "    for epsilon in epsilons:\n",
    "        for rep in range(reps):\n",
    "            agent = DQN_Agent(gamma=gamma, epsilon=epsilon, alpha=alpha, input_dims=env.observation_space.shape[0], n_actions=env.action_space.n, batch_size=64)\n",
    "            reward_per_ep = []\n",
    "            for ep in range(n_episodes):\n",
    "                score = training_loop_DQN(agent)\n",
    "                reward_per_ep.append(score)\n",
    "                avg_score = np.mean(reward_per_ep[max(0,ep-100):(ep+1)])\n",
    "                if ep%10 == 0:\n",
    "                    agent.update_epsilon()\n",
    "                    # print('episode:', ep, 'score:', score, 'avg score:', avg_score)\n",
    "\n",
    "        rewards_per_rep.append(reward_per_ep)\n",
    "        avgs_from_last_hunderd.append(avg_score)\n",
    "        print('Finished rep:', rep)\n",
    "\n",
    "        avg_rewards_per_eps = np.mean(rewards_per_rep, axis=0)\n",
    "        all_avgs_from_last_hunderd = np.mean(avgs_from_last_hunderd, axis=0)\n",
    "\n",
    "        comparison_plot = ComparisonPlot(title=\"Reward per episode for a DQN agent on the lunar lander task\")\n",
    "        comparison_plot.add_curve(np.arange(n_episodes), y=smooth(avg_rewards_per_eps, 10), label='Reward per episode, Alpha: %s epsilon: %s' % (alpha, epsilon))\n",
    "        comparison_plot.add_curve(np.arange(n_episodes), y=all_avgs_from_last_hunderd, label='Average reward from last 100 episodes, Alpha: %s' % (alpha, epsilon))\n",
    "        comparison_plot.save('dqn alpha: %s, epsilon: %s', (alpha, epsilon))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d032e57-3aa6-4690-8969-1271dbd90b5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c45daf-d659-4752-8984-65eb3d30824d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}