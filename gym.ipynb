{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e78a7aa-3e27-4d95-8c9e-19c137429c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install OpenAI Gym dependencies\n",
    "# !pip install Box2D\n",
    "# !pip install box2d-py\n",
    "# !pip install gym[all]\n",
    "# !pip install Box2D\n",
    "# !pip install box2d box2d-kengz\n",
    "# Imports\n",
    "import gym\n",
    "import pygame\n",
    "import random\n",
    "from gym.utils.play import play\n",
    "import numpy as np\n",
    "\n",
    "# DL libraries\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.models import Sequential, load_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "395865b2-e5b7-471d-a824-ff5a1977dbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'LunarLander-v2' # Create environment, source: https://www.gymlibrary.ml/environments\n",
    "env = gym.make(env_name) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "475ff42c-e4a8-4c2d-9bd4-5a17cb2d38f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent class\n",
    "class Agent():\n",
    "    def __init__(self, env):\n",
    "        self.n_actions = env.action_space.n\n",
    "        print(\"Action size:\", self.n_actions)\n",
    "    \n",
    "    def get_action(self, state): \n",
    "        action = random.choice(range(self.n_actions))\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "27a1fc35-5670-4a09-9757-7c34900ffd76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action size: 4\n"
     ]
    }
   ],
   "source": [
    "agent = Agent(env) # Instantiate agent class and pass env\n",
    "state = env.reset() # Reset env and assign starting state\n",
    "\n",
    "for _ in range(200):\n",
    "    action = agent.get_action(state) # Sample action\n",
    "    state, reward, done, info = env.step(action) # Step\n",
    "    env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04033024-1d87-4a60-9290-83b2ea20a5da",
   "metadata": {},
   "source": [
    "## Deep deterministic policy gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f05eaab-5a7b-447f-ace7-fe27ee987963",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OU ACTIN NOISE = https://en.wikipedia.org/wiki/Ornstein%E2%80%93Uhlenbeck_process\n",
    "class Action_noise(object):\n",
    "    \n",
    "    def __init__(self, mu, sigma=0.15, theta=0.2, dt=1e-2, x0=None):\n",
    "        self.theta = theta\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "        self.dt = dt\n",
    "        self.x0 = x0\n",
    "        self.reset()\n",
    "    \n",
    "    def __call__(self):\n",
    "        #TODO: make this more readable\n",
    "        x = self.x_prev + self.theta * (self.mu - self.x_prev) * self.dt\n",
    "        + self.sigma * np.sqrt(self.dt) * np.random.normal(size=self.mu.shape)\n",
    "        self.x_prev = x\n",
    "        return x\n",
    "    \n",
    "    def reset(self):\n",
    "        if self.x0 is not None:\n",
    "            self.x_prev = self.x0\n",
    "        else:\n",
    "            self.x_prev = np.zeros_like(self.mu)\n",
    "    \n",
    "    \n",
    "class ReplayBuffer(object):\n",
    "    \n",
    "    def __init__(self, max_siz, input_shape, n_actions):\n",
    "        self.mem_size = max_size\n",
    "        self.mem_cntr = 0\n",
    "        self.state_memory = np.zeros((self.mem_size, *input_shape))\n",
    "        self.new_state_memory = np.zeros((self.mem_size, *input_shape))\n",
    "        self.action_memory = np.zero((self.mem_size, n_actions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc1abea-72d8-4393-a034-d063fcc8d51f",
   "metadata": {},
   "source": [
    "## Deep Q Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f24e04ed-e910-44b7-89b1-2be5c3255690",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer(object):\n",
    "    \n",
    "    def __init__(self, max_size, input_shape, n_actions):\n",
    "        self.mem_size = max_size\n",
    "        self.mem_counter = 0\n",
    "        self.input_shape = input_shape\n",
    "        self.state_memory = np.zeros((self.mem_size, input_shape))\n",
    "        self.new_state_memory = np.zeros((self.mem_size, input_shape))\n",
    "        dtype = np.int8\n",
    "        self.action_memory = np.zeros((self.mem_size, n_actions), dtype=dtype)\n",
    "        self.reward_memory = np.zeros(self.mem_size)\n",
    "        self.terminal_memory = np.zeros(self.mem_size, dtype=np.float32)\n",
    "    \n",
    "    def store_transition(self, state, action, reward, new_state, done):\n",
    "        index = self.mem_counter % self.mem_size\n",
    "        self.state_memory[index] = state\n",
    "        self.new_state_memory[index] = new_state\n",
    "        self.reward_memory[index] = reward\n",
    "        self.terminal_memory[index] = 1 - int(done)\n",
    "        actions = np.zeros(self.action_memory.shape[1])\n",
    "        actions[action] = 1\n",
    "        self.action_memory[index] = actions\n",
    "        self.mem_counter += 1\n",
    "        \n",
    "    def sample_buffer(self, batch_size):\n",
    "        max_memory = min(self.mem_counter, self.mem_size)\n",
    "        batch = np.random.choice(max_memory, batch_size)\n",
    "        \n",
    "        states = self.state_memory[batch]\n",
    "        new_states = self.new_state_memory[batch]\n",
    "        rewards = self.reward_memory[batch]\n",
    "        actions = self.action_memory[batch]\n",
    "        terminal = self.terminal_memory[batch]\n",
    "        \n",
    "        return states, actions, rewards, new_states, terminal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f297609-01c3-43de-aa21-9ce7e04f1833",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dqn(lr, n_actions, input_dims, fc1_dims, fc2_dims):\n",
    "    model = Sequential([\n",
    "        Dense(fc1_dims, input_shape=(input_dims, )),\n",
    "        Activation('relu'),\n",
    "        Dense(fc2_dims),\n",
    "        Activation('relu'),\n",
    "        Dense(n_actions)\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    \n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ffd7273-5c28-449c-b331-db8c860f6edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "    def __init__(self, alpha, gamma, n_actions, epsilon,\n",
    "                 batch_size, input_dims, mem_size=1000000):\n",
    "        self.action_space = np.arange(n_actions)\n",
    "        self.gamma = gamma \n",
    "        self.epsilon = epsilon\n",
    "        self.alpha = alpha\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.memory = ReplayBuffer(mem_size, input_dims, n_actions)\n",
    "        \n",
    "        self.q_eval = build_dqn(alpha, n_actions, input_dims, fc1_dims=256, fc2_dims=256)\n",
    "        \n",
    "    def remember(self, state, action, reward, new_state, done):\n",
    "        self.memory.store_transition(state, action, reward, new_state, done)\n",
    "        \n",
    "    def select_action(self,state):\n",
    "        state = state[np.newaxis, :] # Fix this into different solution\n",
    "        \n",
    "        if np.random.random() < self.epsilon:\n",
    "            action = np.random.choice(self.action_space)\n",
    "        else:\n",
    "            actions = self.q_eval.predict(state)\n",
    "            action = np.argmax(actions)\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    \n",
    "    def learn(self):\n",
    "        if self.memory.mem_counter < self.batch_size:\n",
    "            return\n",
    "        state, action, reward, new_state, done = self.memory.sample_buffer(self.batch_size)\n",
    "        \n",
    "        action_values = np.array(self.action_space, dtype=np.int8)\n",
    "        action_indices = np.dot(action, action_values)\n",
    "        \n",
    "        \n",
    "        q_eval = self.q_eval.predict(state)\n",
    "        q_next = self.q_eval.predict(new_state)\n",
    "        \n",
    "        q_target = q_eval.copy()\n",
    "        \n",
    "        batch_index = np.arange(self.batch_size, dtype=np.int32)\n",
    "        td = reward + self.gamma * np.argmax(q_eval, axis=1)*done\n",
    "        \n",
    "        q_target[batch_index, action_indices] = td \n",
    "        \n",
    "        _ = self.q_eval.fit(state, q_target, verbose=0) \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e41191-adc9-40b2-a273-d5d1d8a3d977",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-26 13:40:10.318845: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0 score: -47.46484308701936 avg score: -47.46484308701936\n",
      "episode: 1 score: -390.41372714862183 avg score: -218.9392851178206\n",
      "episode: 2 score: -388.5405597279773 avg score: -275.47304332120615\n",
      "episode: 3 score: -223.89711063754453 avg score: -262.57906015029073\n",
      "episode: 4 score: -126.7909455450974 avg score: -235.42143722925206\n",
      "episode: 5 score: -164.53477143093636 avg score: -223.6069929295328\n",
      "episode: 6 score: -85.10293986001446 avg score: -203.8206996338873\n",
      "episode: 7 score: -185.56695896706148 avg score: -201.53898205053406\n",
      "episode: 8 score: -213.32680943457092 avg score: -202.84874064876036\n",
      "episode: 9 score: -309.2935324454599 avg score: -213.4932198284303\n",
      "episode: 10 score: -67.89996178325342 avg score: -200.25746909705057\n",
      "episode: 11 score: -199.76625786188362 avg score: -200.21653482745333\n",
      "episode: 12 score: -100.77233882651353 avg score: -192.56698128891952\n",
      "episode: 13 score: -357.0734686548025 avg score: -204.31744467219687\n",
      "episode: 14 score: -104.72036726869653 avg score: -197.67763951196352\n",
      "episode: 15 score: -23.071326174758127 avg score: -186.7647449283882\n",
      "episode: 16 score: -222.48031348926418 avg score: -188.8656607260868\n",
      "episode: 17 score: -64.5810809548804 avg score: -181.96096184990864\n",
      "episode: 18 score: 42.728523397008956 avg score: -170.13519946849195\n",
      "episode: 19 score: -183.23334373686623 avg score: -170.79010668191066\n",
      "episode: 20 score: -126.78674268254649 avg score: -168.69470839622664\n",
      "episode: 21 score: -153.44589065396235 avg score: -168.0015803170328\n",
      "episode: 22 score: -94.20634203198645 avg score: -164.79309169594382\n",
      "episode: 23 score: -80.28406554129147 avg score: -161.27188227283332\n",
      "episode: 24 score: -184.1211426493462 avg score: -162.18585268789386\n",
      "episode: 25 score: -10.733664259680836 avg score: -156.36076851757798\n",
      "episode: 26 score: -103.90232734791786 avg score: -154.41786328907205\n",
      "episode: 27 score: -77.67008985813655 avg score: -151.67687138082437\n",
      "episode: 28 score: -120.20348370883009 avg score: -150.5915821507556\n",
      "episode: 29 score: -26.40871453361268 avg score: -146.45215323018417\n",
      "episode: 30 score: -80.22237289553964 avg score: -144.31570870326016\n",
      "episode: 31 score: -95.20395438977067 avg score: -142.78096638096358\n",
      "episode: 32 score: 85.96562016677524 avg score: -135.84925163709272\n",
      "episode: 33 score: 126.07138511513384 avg score: -128.14570349732134\n",
      "episode: 34 score: -26.22715512510102 avg score: -125.23374497240074\n",
      "episode: 35 score: -72.93594269638001 avg score: -123.78102824251128\n",
      "episode: 36 score: -18.518575976528837 avg score: -120.93609710018744\n",
      "episode: 37 score: -122.35728979017414 avg score: -120.97349690781867\n",
      "episode: 38 score: -253.1237005489051 avg score: -124.36196366784651\n",
      "episode: 39 score: -119.30629767437412 avg score: -124.23557201800972\n",
      "episode: 40 score: -78.09030420293249 avg score: -123.11007768105661\n",
      "episode: 41 score: -36.79116331206305 avg score: -121.05486543417581\n",
      "episode: 42 score: 241.90943834744053 avg score: -112.61383511367312\n",
      "episode: 43 score: -95.23445240182525 avg score: -112.2188491429493\n",
      "episode: 44 score: -102.75045288904211 avg score: -112.00844033730692\n",
      "episode: 45 score: -80.5392003773882 avg score: -111.32432642513479\n",
      "episode: 46 score: -163.5793877426342 avg score: -112.43613624040073\n",
      "episode: 47 score: -291.70561654049004 avg score: -116.17091707998593\n",
      "episode: 48 score: -223.08313781766293 avg score: -118.35279913585687\n",
      "episode: 49 score: -209.70751302509183 avg score: -120.17989341364157\n",
      "episode: 50 score: -163.60987948308565 avg score: -121.03146176794439\n",
      "episode: 51 score: -94.89374385074423 avg score: -120.52881334645978\n",
      "episode: 52 score: -450.5901002799793 avg score: -126.75638479803563\n",
      "episode: 53 score: -234.6751424158013 avg score: -128.75488030947574\n",
      "episode: 54 score: -13.049443753818082 avg score: -126.65114509937285\n",
      "episode: 55 score: -63.047401175048776 avg score: -125.51536395786707\n",
      "episode: 56 score: -252.75617385981536 avg score: -127.74765886842756\n",
      "episode: 57 score: -236.67346910792918 avg score: -129.62569007945345\n",
      "episode: 58 score: -5.319702172913992 avg score: -127.51880892849515\n",
      "episode: 59 score: -162.21543341976178 avg score: -128.0970860033496\n",
      "episode: 60 score: -82.01844898821531 avg score: -127.3416985112982\n",
      "episode: 61 score: -319.4626819844881 avg score: -130.44042405118836\n",
      "episode: 62 score: 24.701492798444292 avg score: -127.97785394246404\n",
      "episode: 63 score: -130.44474914209337 avg score: -128.01639917995826\n",
      "episode: 64 score: -96.57325285247035 avg score: -127.53265846722766\n",
      "episode: 65 score: -215.19853718614624 avg score: -128.86092935690826\n",
      "episode: 66 score: -270.82647270195406 avg score: -130.97981806355074\n",
      "episode: 67 score: -5.596355206758943 avg score: -129.13594360977436\n",
      "episode: 68 score: -66.83761533709219 avg score: -128.23306928698187\n",
      "episode: 69 score: -70.16714034724627 avg score: -127.40355601641423\n",
      "episode: 70 score: -66.1502095935179 avg score: -126.54083282735934\n",
      "episode: 71 score: -88.96422923848 avg score: -126.01893555529159\n",
      "episode: 72 score: 67.47590858352542 avg score: -123.36832125202014\n",
      "episode: 73 score: -74.06803637787284 avg score: -122.70210118615327\n",
      "episode: 74 score: -151.0355414992502 avg score: -123.07988039032789\n",
      "episode: 75 score: -90.46021487167758 avg score: -122.65067426508249\n",
      "episode: 76 score: -77.79480653834258 avg score: -122.06813052837157\n",
      "episode: 77 score: -163.61011066481512 avg score: -122.60072001730033\n",
      "episode: 78 score: -35.23516974838337 avg score: -121.49482697592163\n",
      "episode: 79 score: -154.23110919279367 avg score: -121.90403050363257\n",
      "episode: 80 score: -250.0895970572528 avg score: -123.48656836231923\n",
      "episode: 81 score: -245.77460805836392 avg score: -124.97788591958808\n",
      "episode: 82 score: -230.30946018723364 avg score: -126.24694103124646\n",
      "episode: 83 score: -6.8412368512562125 avg score: -124.82544455291324\n",
      "episode: 84 score: 162.3534215815409 avg score: -121.44686965721378\n",
      "episode: 85 score: -208.34958263768817 avg score: -122.45736631977742\n",
      "episode: 86 score: -168.5875653011314 avg score: -122.9875984919769\n",
      "episode: 87 score: -220.24637000566406 avg score: -124.09281180463243\n",
      "episode: 88 score: -257.83736150129073 avg score: -125.59555955403309\n",
      "episode: 89 score: 55.83310259854028 avg score: -123.5796855301156\n",
      "episode: 90 score: -26.066554098599127 avg score: -122.5081126572418\n",
      "episode: 91 score: -166.8276347212959 avg score: -122.98984659272064\n",
      "episode: 92 score: -57.7783336077266 avg score: -122.28864752836587\n",
      "episode: 93 score: 28.317574269960232 avg score: -120.68645367944751\n",
      "episode: 94 score: -20.32460380471973 avg score: -119.63001315445037\n",
      "episode: 95 score: -293.11094354437597 avg score: -121.43710617934543\n",
      "episode: 96 score: -89.25292393522949 avg score: -121.10531048610713\n",
      "episode: 97 score: 51.99862132696768 avg score: -119.3389438349533\n",
      "episode: 98 score: -233.6282657351309 avg score: -120.49338142990459\n",
      "episode: 99 score: -268.5757649207733 avg score: -121.97420526481328\n",
      "episode: 100 score: -250.9657575430511 avg score: -123.25134934677604\n",
      "episode: 101 score: -75.36330463551786 avg score: -123.52757173834532\n",
      "episode: 102 score: -210.37526557557072 avg score: -121.74501271286957\n"
     ]
    }
   ],
   "source": [
    "agent = Agent(gamma=0.99, epsilon=0.2, alpha=0.001, input_dims=8, n_actions=4, batch_size=64)\n",
    "\n",
    "scores = []\n",
    "eps_history = [ ]\n",
    "n_episodes = 200\n",
    "\n",
    "for ep in range(n_episodes):\n",
    "    done = False\n",
    "    score = 0\n",
    "    state = env.reset()\n",
    "    \n",
    "    while not done:\n",
    "        action = agent.select_action(state)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        score += reward\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        agent.learn()\n",
    "        env.render()\n",
    "        \n",
    "        \n",
    "        \n",
    "    scores.append(score)\n",
    "    \n",
    "    avg_score = np.mean(scores[max(0,ep-100):(ep+1)])\n",
    "    print('episode:', ep, 'score:', score, 'avg score:', avg_score)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5c154a-f9fa-4474-b2bb-851cb1568114",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d032e57-3aa6-4690-8969-1271dbd90b5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c45daf-d659-4752-8984-65eb3d30824d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
